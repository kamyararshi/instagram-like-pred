{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate profiles data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  alias  numberPosts  numberFollowers  numberFollowing  \\\n",
      "0            1misssmeis          988           720979              233   \n",
      "1                3ala2o          938           792886              466   \n",
      "2                   433         6009         14545102              433   \n",
      "3        6senseofficial         3324           243094                0   \n",
      "4               7ikhals         1444           219458              221   \n",
      "..                  ...          ...              ...              ...   \n",
      "973             _ingo_1          422           149566              127   \n",
      "974  _mariannejacobsen_         1593           189279              290   \n",
      "975             _picolo          776           927457              566   \n",
      "976          _tinamaria          821           160393              730   \n",
      "977              _tuck4         1623           139150              246   \n",
      "\n",
      "                                               website  \n",
      "0                                    www.sylviemeis.de  \n",
      "1                  www.youtube.com/watch?v=MXkqzeIlhSQ  \n",
      "2                                    onelink.to/q6w524  \n",
      "3                      www.facebook.com/6senseofficial  \n",
      "4                                 instagram.com/ikhals  \n",
      "..                                                 ...  \n",
      "973                                               None  \n",
      "974                          www.delicate-photoart.com  \n",
      "975                             www.patreon.com/picolo  \n",
      "976  jewelrybyad.com/da/jewelry-for-a-good-cause-20...  \n",
      "977                                   takashiyasui.com  \n",
      "\n",
      "[978 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the folder path\n",
    "folder_path = \"profiles\"\n",
    "\n",
    "# Initialize empty lists to store the extracted data\n",
    "alias_list = []\n",
    "number_posts_list = []\n",
    "number_followers_list = []\n",
    "number_following_list = []\n",
    "website_list = []\n",
    "\n",
    "# Iterate over the JSON files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        # Read the JSON file with UTF-8 encoding\n",
    "        with open(os.path.join(folder_path, filename), encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Extract the required attributes\n",
    "        alias = data.get(\"alias\")\n",
    "        number_posts = data.get(\"numberPosts\")\n",
    "        number_followers = data.get(\"numberFollowers\")\n",
    "        number_following = data.get(\"numberFollowing\")\n",
    "        website = data.get(\"website\")\n",
    "        \n",
    "        # Append the data to the respective lists\n",
    "        alias_list.append(alias)\n",
    "        number_posts_list.append(number_posts)\n",
    "        number_followers_list.append(number_followers)\n",
    "        number_following_list.append(number_following)\n",
    "        website_list.append(website)\n",
    "\n",
    "# Create the data frame\n",
    "data = {\n",
    "    \"alias\": alias_list,\n",
    "    \"numberPosts\": number_posts_list,\n",
    "    \"numberFollowers\": number_followers_list,\n",
    "    \"numberFollowing\": number_following_list,\n",
    "    \"website\": website_list\n",
    "}\n",
    "profiles_df = pd.DataFrame(data)\n",
    "\n",
    "# Print the data frame\n",
    "print(profiles_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate posts data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            alias                                           urlImage  isVideo  \\\n",
      "0      1misssmeis  https://scontent.cdninstagram.com/t51.2885-15/...    False   \n",
      "1      1misssmeis  https://scontent.cdninstagram.com/t51.2885-15/...    False   \n",
      "2      1misssmeis  https://scontent.cdninstagram.com/t51.2885-15/...    False   \n",
      "3      1misssmeis  https://scontent.cdninstagram.com/t51.2885-15/...    False   \n",
      "4      1misssmeis  https://scontent.cdninstagram.com/t51.2885-15/...    False   \n",
      "...           ...                                                ...      ...   \n",
      "16534      _tuck4  https://scontent.cdninstagram.com/t51.2885-15/...    False   \n",
      "16535      _tuck4  https://scontent.cdninstagram.com/t51.2885-15/...    False   \n",
      "16536      _tuck4  [https://scontent.cdninstagram.com/t51.2885-15...    False   \n",
      "16537      _tuck4  https://scontent.cdninstagram.com/t51.2885-15/...    False   \n",
      "16538      _tuck4  https://scontent.cdninstagram.com/t51.2885-15/...    False   \n",
      "\n",
      "       multipleImage                                    tags  \\\n",
      "0              False                      [#exclusivspezial]   \n",
      "1              False               [#jumpsuit, #glam, #ootn]   \n",
      "2              False       [#rehearsalday, #behindthescenes]   \n",
      "3              False                 [#dress, #shoes, #glam]   \n",
      "4              False  [#weekend, #yay, #happyweekend, #kiss]   \n",
      "...              ...                                     ...   \n",
      "16534          False                              [#RECO_ig]   \n",
      "16535          False                              [#RECO_ig]   \n",
      "16536           True                                 [#film]   \n",
      "16537          False                              [#RECO_ig]   \n",
      "16538          False                              [#RECO_ig]   \n",
      "\n",
      "                                                mentions  \\\n",
      "0      [@fraukeludowig_official, @rtl_exclusiv, @tine...   \n",
      "1      [@tine, @rachelzoe, @lecolook, @letsdance, @rt...   \n",
      "2               [@letsdance, @rtlde, @rtlde, @letsdance]   \n",
      "3      [@bydanienl, @roland_mouret, @casadeiofficial,...   \n",
      "4                [@elisabettafranchi, @serenagoldenbaum]   \n",
      "...                                                  ...   \n",
      "16534                                         [@reco_ig]   \n",
      "16535                                         [@reco_ig]   \n",
      "16536                                                 []   \n",
      "16537                                         [@reco_ig]   \n",
      "16538                                         [@reco_ig]   \n",
      "\n",
      "                                             description  \\\n",
      "0      With my lovely colleague @fraukeludowig_offici...   \n",
      "1      My look last night, hosting Let's Dance! Style...   \n",
      "2      Calm before the 'glam' storm! üíãüíÑüíÖüèªüíÜüèºüíáüèºTomorrow...   \n",
      "3      üåºToday's look for QVC.. styled by @bydanienl #...   \n",
      "4      üíãHappy Weekend Lovelies ‚ù§ #weekend #yay #happy...   \n",
      "...                                                  ...   \n",
      "16534                   Everyday life in Tokyo\\n#RECO_ig   \n",
      "16535                   Everyday life in Tokyo\\n#RECO_ig   \n",
      "16536          Everyday life in Tokyo #film\\n#ÂÜô„É´„É≥„Åß„Åô #‰∏≠Â§ÆÁ∑ö   \n",
      "16537                   Everyday life in Tokyo\\n#RECO_ig   \n",
      "16538                          Tokyo in spring\\n#RECO_ig   \n",
      "\n",
      "                           date  numberLikes  \n",
      "0      2017-04-29T05:00:00.000Z        10047  \n",
      "1      2017-04-29T05:00:00.000Z        16781  \n",
      "2      2017-04-27T05:00:00.000Z        11227  \n",
      "3      2017-04-24T05:00:00.000Z        21539  \n",
      "4      2017-04-22T05:00:00.000Z        21054  \n",
      "...                         ...          ...  \n",
      "16534  2017-04-13T05:00:00.000Z        12240  \n",
      "16535  2017-04-12T05:00:00.000Z        11804  \n",
      "16536  2017-04-11T05:00:00.000Z         6814  \n",
      "16537  2017-04-10T05:00:00.000Z        13860  \n",
      "16538  2017-04-09T05:00:00.000Z        14921  \n",
      "\n",
      "[16539 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the folder path\n",
    "folder_path = \"profiles\"\n",
    "\n",
    "# Initialize empty lists to store the extracted data\n",
    "alias_list = []\n",
    "url_image_list = []\n",
    "is_video_list = []\n",
    "multiple_image_list = []\n",
    "tags_list = []\n",
    "mentions_list = []\n",
    "description_list = []\n",
    "date_list = []\n",
    "number_likes_list = []\n",
    "\n",
    "# Iterate over the JSON files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        # Read the JSON file with UTF-8 encoding\n",
    "        with open(os.path.join(folder_path, filename), encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Extract the alias\n",
    "        alias = data.get(\"alias\")\n",
    "        \n",
    "        # Extract the posts\n",
    "        posts = data.get(\"posts\", [])\n",
    "        for post in posts:\n",
    "            url_image = post.get(\"urlImage\")\n",
    "            is_video = post.get(\"isVideo\")\n",
    "            multiple_image = post.get(\"multipleImage\")\n",
    "            tags = post.get(\"tags\")\n",
    "            mentions = post.get(\"mentions\")\n",
    "            description = post.get(\"description\")\n",
    "            date = post.get(\"date\")\n",
    "            number_likes = post.get(\"numberLikes\")\n",
    "            \n",
    "            # Append the data to the respective lists\n",
    "            alias_list.append(alias)\n",
    "            url_image_list.append(url_image)\n",
    "            is_video_list.append(is_video)\n",
    "            multiple_image_list.append(multiple_image)\n",
    "            tags_list.append(tags)\n",
    "            mentions_list.append(mentions)\n",
    "            description_list.append(description)\n",
    "            date_list.append(date)\n",
    "            number_likes_list.append(number_likes)\n",
    "\n",
    "# Create the data frame\n",
    "data = {\n",
    "    \"alias\": alias_list,\n",
    "    \"urlImage\": url_image_list,\n",
    "    \"isVideo\": is_video_list,\n",
    "    \"multipleImage\": multiple_image_list,\n",
    "    \"tags\": tags_list,\n",
    "    \"mentions\": mentions_list,\n",
    "    \"description\": description_list,\n",
    "    \"date\": date_list,\n",
    "    \"numberLikes\": number_likes_list\n",
    "}\n",
    "posts_df = pd.DataFrame(data)\n",
    "\n",
    "# Print the data frame\n",
    "print(posts_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average number of likes per post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  alias  numberPosts  numberFollowers  numberFollowing  \\\n",
      "0            1misssmeis          988           720979              233   \n",
      "1                3ala2o          938           792886              466   \n",
      "2                   433         6009         14545102              433   \n",
      "3        6senseofficial         3324           243094                0   \n",
      "4               7ikhals         1444           219458              221   \n",
      "..                  ...          ...              ...              ...   \n",
      "973             _ingo_1          422           149566              127   \n",
      "974  _mariannejacobsen_         1593           189279              290   \n",
      "975             _picolo          776           927457              566   \n",
      "976          _tinamaria          821           160393              730   \n",
      "977              _tuck4         1623           139150              246   \n",
      "\n",
      "                                               website  average_likes  \n",
      "0                                    www.sylviemeis.de   23400.941176  \n",
      "1                  www.youtube.com/watch?v=MXkqzeIlhSQ    9300.058824  \n",
      "2                                    onelink.to/q6w524  310683.647059  \n",
      "3                      www.facebook.com/6senseofficial    9453.941176  \n",
      "4                                 instagram.com/ikhals    1074.352941  \n",
      "..                                                 ...            ...  \n",
      "973                                               None    2133.588235  \n",
      "974                          www.delicate-photoart.com    2834.882353  \n",
      "975                             www.patreon.com/picolo   81758.764706  \n",
      "976  jewelrybyad.com/da/jewelry-for-a-good-cause-20...    4076.352941  \n",
      "977                                   takashiyasui.com   10355.294118  \n",
      "\n",
      "[978 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Group posts_df by alias and calculate the average numberLikes\n",
    "average_likes = posts_df.groupby('alias')['numberLikes'].mean()\n",
    "\n",
    "# Add the average_likes column to profiles_df\n",
    "profiles_df['average_likes'] = profiles_df['alias'].map(average_likes)\n",
    "\n",
    "# Display the updated profiles_df\n",
    "print(profiles_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(747, 6)\n",
      "(12697, 9)\n"
     ]
    }
   ],
   "source": [
    "#filtering, such that\n",
    "#numberFollowers < 1.000.000\n",
    "#average_likes  < 200.000\n",
    "profiles_df = profiles_df[(profiles_df['numberFollowers'] < 1000000) & (profiles_df['average_likes'] < 200000)]\n",
    "print(profiles_df.shape)\n",
    "\n",
    "#apply filters also to posts_df\n",
    "posts_df = posts_df[posts_df['alias'].isin(profiles_df['alias'])]\n",
    "print(posts_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing profiles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    660\n",
       "0     87\n",
       "Name: website_available, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assigning categories based on the website\n",
    "def website_available(website):\n",
    "    if pd.isnull(website):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "profiles_df['website_available'] = profiles_df['website'].apply(website_available)\n",
    "\n",
    "profiles_df['website_available'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing posts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing day of the week\n",
    "posts_df['date'] = pd.to_datetime(posts_df['date'])\n",
    "posts_df['weekday'] = posts_df['date'].dt.strftime('%A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likes categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     1272\n",
       "9     1271\n",
       "5     1271\n",
       "6     1270\n",
       "4     1270\n",
       "10    1269\n",
       "8     1269\n",
       "3     1269\n",
       "7     1268\n",
       "2     1268\n",
       "Name: numberLikesCategory, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Categorizing into 10 equally sized groups based on numberLikes\n",
    "#Category 10 are the 10% of posts with the highest likes\n",
    "#Category 1 are the 10% with the lowest likes\n",
    "\n",
    "# Sort the DataFrame by numberLikes in descending order\n",
    "posts_df = posts_df.sort_values('numberLikes', ascending=False)\n",
    "\n",
    "# Calculate the quantiles for the groups\n",
    "quantiles = pd.qcut(posts_df['numberLikes'], q=10, labels=False, duplicates='drop')\n",
    "\n",
    "# Assign the group numbers to the numberLikesCategory column\n",
    "posts_df['numberLikesCategory'] = quantiles + 1  # Add 1 to make the group numbers start from 1 instead of 0\n",
    "\n",
    "posts_df['numberLikesCategory'].value_counts()\n",
    "#sorted_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of relevant hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#love', '#instagood', '#instagram', '#fashion', '#photooftheday', '#beautiful', '#art', '#photography', '#happy', '#picoftheday', '#cute', '#follow', '#tbt', '#followme', '#nature', '#likelike', '#travel', '#style', '#repost', '#summer', '#instadaily', '#selfie', '#me', '#music', '#friends', '#fitness', '#girl', '#food', '#fun', '#beauty', '#instalike', '#smile', '#family', '#photo', '#life', '#likeforlike', '#ootd', '#followfollow', '#makeup', '#amazing', '#igers', '#nofilter', '#dog', '#model', '#sunset', '#beach', '#instamood', '#foodporn', '#motivation', '#followforfollow', '#design', '#lifestyle', '#sky', '#ll', '#ff', '#ÏùºÏÉÅ', '#cat', '#handmade', '#hair', '#nails', '#vscocam', '#bestoftheday', '#vsco', '#funny', '#dogsofinstagram', '#drawing', '#artist', '#gym', '#flowers', '#baby', '#wedding', '#girls', '#instapic', '#pretty', '#likeforlikes', '#photographer', '#instafood', '#party', '#inspiration', '#lol', '#cool', '#workout', '#likeforfollow', '#swag', '#fit', '#healthy', '#yummy', '#blackandwhite', '#tiktok', '#foodie', '#moda', '#home', '#christmas', '#black', '#memes', '#holiday', '#pink', '#sea', '#landscape', '#blue', '#london', '#winter', '#night', '#puppy', '#catsofinstagram', '#tattoo', '#happiness', '#instafashion', '#work', '#illustration', '#architecture', '#japan', '#weekend', '#daily', '#explore', '#nyc', '#coffee', '#look', '#Îç∞ÏùºÎ¶¨', '#blessed', '#spring', '#italy', '#travelgram', '#ÎßûÌåî', '#portrait', '#ÏÜåÌÜµ', '#trip', '#shopping', '#paris', '#dress', '#tflers', '#vacation', '#wanderlust', '#health', '#goodmorning', '#fashionblogger', '#anime', '#red', '#vintage', '#travelphotography', '#green', '#sweet', '#meme', '#instalove', '#insta', '#birthday', '#instafollow', '#luxury', '#throwback', '#amor', '#followback', '#sketch', '#fitfam', '#delicious', '#bts', '#dogs', '#boy', '#new', '#instacool', '#clouds', '#relax', '#outfit', '#naturephotography', '#photoshoot', '#likes', '#shoes', '#sunday', '#bodybuilding', '#artwork', '#cats', '#indonesia', '#instatravel', '#training', '#adventure', '#quotes', '#dance', '#followforfollowback', '#fashionista', '#dinner', '#usa', '#newyork', '#nike', '#painting', '#canon', '#picture', '#morning', '#blogger', '#iphoneonly', '#–º–æ—Å–∫–≤–∞', '#awesome', '#dubai', '#istanbul', '#vegan', '#mood', '#instaphoto', '#breakfast', '#nice', '#interiordesign', '#color', '#white', '#pet', '#snow', '#likelikes', '#tweegram', '#live', '#eyes', '#Ï¢ãÏïÑÏöî', '#kids', '#igdaily', '#ÿßŸÑŸÉŸàŸäÿ™', '#pic', '#goals', '#my', '#instago', '#kpop', '#flower', '#lfl', '#webstagram', '#goodvibes', '#happybirthday', '#autumn', '#memories', '#halloween', '#yum', '#makeupartist', '#lunch', '#dogstagram', '#cake', '#fitnessmotivation', '#city', '#california', '#hiphop', '#peace', '#ÏÑ†Ìåî', '#instagramers', '#yoga', '#colorful', '#ÏÖÄÏπ¥', '#sport', '#gay', '#chocolate', '#streetstyle', '#instadog', '#nikon', '#ÏÖÄÏä§ÌÉÄÍ∑∏Îû®', '#view', '#latepost', '#france', '#streetphotography', '#jewelry', '#fff', '#tb', '#healthyfood', '#homemade', '#loveit', '#tattoos', '#brasil', '#', '#water', '#day', '#india', '#mountains', '#good', '#traveling', '#Î®πÏä§ÌÉÄÍ∑∏Îû®', '#friday', '#light', '#tumblr', '#sale', '#draw', '#ÿ™ÿµŸàŸäÿ±Ÿä', '#ink', '#creative', '#babygirl', '#nailart', '#best', '#all_shots', '#disney', '#miami', '#fitspo', '#video', '#sunshine', '#regrann', '#animals', '#followers', '#car', '#colors', '#foodstagram', '#saturday', '#beer', '#dankmemes', '#instasize', '#mexico', '#thailand', '#adorable', '#ÿßŸÑÿ≥ÿπŸàÿØŸäÿ©', '#likesforlikes', '#familia', '#football', '#likefollow', '#jakarta', '#hairstyle', '#ocean', '#sunrise', '#lovely', '#quote', '#natural', '#chanel', '#stylish', '#top', '#goodnight', '#losangeles', '#homedecor', '#mylove', '#instaart', '#mua', '#naturelovers', '#street', '#weightloss', '#youtube', '#running', '#adidas', '#diet', '#arte', '#travelling', '#business', '#holidays', '#gucci', '#instagramhub', '#entrepreneur', '#goodtimes', '#pets', '#catstagram', '#blonde', '#–∫—Ä–∞—Å–æ—Ç–∞', '#ÌåîÎ°úÏö∞', '#healthylifestyle', '#ÿßŸÑÿ±Ÿäÿßÿ∂', '#australia', '#ÏÖÄÌîº', '#bali', '#animal', '#cafe', '#wcw', '#world', '#russia', '#hijab', '#venezuela', '#bomdia', '#bestfriend', '#aesthetic', '#bestfriends', '#fresh', '#bff', '#designer', '#onlineshop', '#eatclean', '#spain', '#decor', '#', '#kuwait', '#success', '#fall', '#film', '#canada', '#ÿßŸÑÿßŸÖÿßÿ±ÿßÿ™', '#uae', '#', '#onedirection', '#germany', '#bride', '#strong', '#tree', '#perfect', '#likes', '#gold', '#babyboy', '#digitalart', '#colombia', '#outdoors', '#couple', '#muscle', '#foodphotography', '#explorepage', '#travelblogger', '#passion', '#body', '#gymlife', '#bored', '#fotografia', '#loveyourself', '#barcelona', '#crossfit', '#ÿßŸÑÿ®ÿ≠ÿ±ŸäŸÜ', '#instamoment', '#wine', '#cars', '#enjoy', '#friendship', '#bajumurah', '#interior', '#skincare', '#diy', '#dessert', '#turkey', '#hiking', '#lmao', '#garden', '#polishgirl', '#moscow', '#roadtrip', '#ŸÇÿ∑ÿ±', '#streetart', '#florida', '#petstagram', '#italia', '#woman', '#kitty', '#foodgasm', '#friend', '#instafit', '#outfitoftheday', '#la', '#europe', '#accessories', '#rap', '#boanoite', '#', '#truth', '#forever', '#trees', '#shoutout', '#loveyou', '#dank', '#uk', '#familytime', '#sundayfunday', '#mensfashion', '#rock', '#exercise', '#tokyo', '#mylife', '#photos', '#money', '#throwbackthursday', '#Îç∞ÏùºÎ¶¨Î£©', '#sisters', '#foto', '#momlife', '#potd', '#women', '#latergram', '#loveher', '#gift', '#likeback', '#nail', '#gorgeous', '#funnymemes', '#—Ä–æ—Å—Å–∏—è', '#doglover', '#instastyle', '#sunny', '#dior', '#bhfyp', '#positivevibes', '#puppylove', '#bandung', '#shop', '#bmw', '#viral', '#ÿØÿ®Ÿä', '#singer', '#abs', '#crazy', '#korea', '#jimin', '#sister', '#run', '#fitnessmodel', '#nutrition', '#monday', '#comment', '#tasty', '#sketchbook', '#mcm', '#book', '#instacat', '#cleaneating', '#chicago', '#artistsoninstagram', '#', '#jungkook', '#i', '#chile', '#pizza', '#blog', '#dj', '#summertime', '#onlineshopping', '#dogoftheday']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "hashtags = []\n",
    "\n",
    "#reading hashtags from textfile, adding them to a list of strings\n",
    "with open('top_500_hashtags.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        hashtags.append(line.strip())\n",
    "\n",
    "cleaned_hashtags = []\n",
    "\n",
    "#cleaning the strings, i.e. remove index infront of the hashtag an popularity after it\n",
    "for hashtag in hashtags:\n",
    "    cleaned_hashtag = re.sub(r'[0-9.]', '', hashtag)[:-1]\n",
    "    cleaned_hashtags.append(cleaned_hashtag)\n",
    "\n",
    "print(cleaned_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     11006\n",
       "1       916\n",
       "2       281\n",
       "3       200\n",
       "4       127\n",
       "5        65\n",
       "8        25\n",
       "6        20\n",
       "7        16\n",
       "10        9\n",
       "11        9\n",
       "9         8\n",
       "12        4\n",
       "15        3\n",
       "13        3\n",
       "14        2\n",
       "17        1\n",
       "16        1\n",
       "19        1\n",
       "Name: amount_relevant_tags, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#amount of hastags over all\n",
    "posts_df['amount_tags'] = posts_df['tags'].apply(lambda x: len(x))\n",
    "\n",
    "# Function to count relevant tags\n",
    "def count_relevant_tags(tags, cleaned_hashtags):\n",
    "    return sum(tag in cleaned_hashtags for tag in tags)\n",
    "\n",
    "# Apply the function to each row and store the result in a new column\n",
    "posts_df['amount_relevant_tags'] = posts_df.apply(lambda row: count_relevant_tags(row['tags'], cleaned_hashtags), axis=1)\n",
    "\n",
    "posts_df['amount_relevant_tags'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns rows with same alias and smaller date than date in parameter\n",
    "def filter_dataframe_by_alias_and_date(df, alias, date):\n",
    "    # Convert the date parameter to a Timestamp object\n",
    "    date = pd.Timestamp(date)\n",
    "\n",
    "    # Filter the DataFrame based on the given conditions\n",
    "    filtered_df = df[(df['alias'] == alias) & (df['date'] < date)]\n",
    "\n",
    "    # Return the filtered DataFrame\n",
    "    return filtered_df.sort_values('date', ascending=False)\n",
    "\n",
    "def moving_average(df, row):\n",
    "    filtered_df = filter_dataframe_by_alias_and_date(df, row['alias'], row['date'])\n",
    "    \n",
    "    if len(filtered_df) < 5:\n",
    "        return -1\n",
    "    else:\n",
    "        return filtered_df.head(5)['numberLikes'].mean()\n",
    "\n",
    "    \n",
    "posts_df['moving_avg'] = posts_df.apply(lambda row: moving_average(posts_df, row), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\86178\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # Remove punctuation using regular expressions\n",
    "    no_punct = re.sub('[' + string.punctuation + string.digits + ']', '', text)\n",
    "    return no_punct\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Remove stopwords using NLTK corpus\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    no_stopwords = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "    return no_stopwords\n",
    "\n",
    "def remove_emojis(text):\n",
    "    # Convert emojis to textual representation and remove them\n",
    "    no_emojis = emoji.demojize(text)\n",
    "    no_emojis = re.sub('(:[a-z_-]+:)', ' ', no_emojis, flags=re.IGNORECASE)\n",
    "    return no_emojis\n",
    "posts_df['descriptionProcessed'] = posts_df['description'].apply(remove_punctuation)\n",
    "posts_df['descriptionProcessed'] = posts_df['descriptionProcessed'].apply(remove_stopwords)\n",
    "posts_df['descriptionProcessed'] = posts_df['descriptionProcessed'].apply(remove_emojis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_embeddings(file=\"wiki-news-300d-1M.vec\"):\n",
    "    embeddings = {}\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 40000:\n",
    "                break\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            embeddings[tokens[0]] = np.asarray(tokens[1:], dtype='float64')\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def embed_sentence(sentence, word2emb):\n",
    "    tokens = tokenize(sentence)\n",
    "    token_embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in word2emb:\n",
    "            token_embeddings.append(word2emb[token])\n",
    "        else:\n",
    "            token_embeddings.append(np.zeros(300))\n",
    "    if len(token_embeddings) > 0:\n",
    "        sentence_embedding = np.mean(token_embeddings, axis=0)\n",
    "    else:\n",
    "        sentence_embedding = np.zeros(300)\n",
    "    return sentence_embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = load_embeddings()\n",
    "posts_df['descriptionVector'] = None\n",
    "posts_df['descriptionVector'] = \\\n",
    "    [embed_sentence(sentence, embeddings) for sentence in posts_df['descriptionProcessed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12697, 500)\n",
      "(12697, 16)\n"
     ]
    }
   ],
   "source": [
    "#by default the vectorizer conerts the text to lower case and uses word-level tokenization\n",
    "# Create an instance of CountVectorizer with max_features set to 500 (this is what they did in the tds implementation)\n",
    "vec = CountVectorizer(max_features=500)\n",
    "\n",
    "\n",
    "# Transform the \"descriptionProcessed\" column into a matrix of token counts\n",
    "description_counts = vec.fit_transform(posts_df['descriptionProcessed'])\n",
    "\n",
    "# Convert the matrix to an array\n",
    "description_counts_array = description_counts.toarray()\n",
    "\n",
    "df = pd.DataFrame(data=description_counts_array,columns = vec.get_feature_names_out())\n",
    "print(df.shape)\n",
    "print(posts_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['alias', 'urlImage', 'isVideo', 'multipleImage', 'tags', 'mentions',\n",
       "       'description', 'date', 'numberLikes', 'weekday', 'numberLikesCategory',\n",
       "       'amount_tags', 'amount_relevant_tags', 'moving_avg',\n",
       "       'descriptionProcessed', 'descriptionVector', 'language'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Assuming 'description' is the column name in your DataFrame\n",
    "posts_df['language'] = posts_df['description'].apply(detect_language)\n",
    "\n",
    "\n",
    "posts_df['language'].value_counts()\n",
    "posts_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['alias', 'numberPosts', 'numberFollowers', 'numberFollowing',\n",
       "       'website_available'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_keep = ['alias', 'numberPosts', 'numberFollowers', 'numberFollowing', 'website_available']\n",
    "profiles_df = profiles_df[columns_to_keep]\n",
    "profiles_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alias</th>\n",
       "      <th>multipleImage</th>\n",
       "      <th>weekday</th>\n",
       "      <th>numberLikesCategory</th>\n",
       "      <th>amount_relevant_tags</th>\n",
       "      <th>moving_avg</th>\n",
       "      <th>descriptionProcessed</th>\n",
       "      <th>descriptionVector</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4472</th>\n",
       "      <td>elisabeth.rioux</td>\n",
       "      <td>False</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>119771.0</td>\n",
       "      <td>OFFICIALLY PARENTS    Yes Jonathan came live C...</td>\n",
       "      <td>[-0.015670370370370366, 0.01253703703703704, -...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16495</th>\n",
       "      <td>_picolo</td>\n",
       "      <td>False</td>\n",
       "      <td>Friday</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>70003.8</td>\n",
       "      <td>Going watch Beauty Beast tonight    Hoping wor...</td>\n",
       "      <td>[-0.042608695652173914, -0.024921739130434783,...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4482</th>\n",
       "      <td>elisabeth.rioux</td>\n",
       "      <td>False</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>111266.4</td>\n",
       "      <td>days Ill way see love life India hes reason ha...</td>\n",
       "      <td>[-0.04034375, -0.010975, -0.04198125, -0.05997...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4480</th>\n",
       "      <td>elisabeth.rioux</td>\n",
       "      <td>False</td>\n",
       "      <td>Monday</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>116196.8</td>\n",
       "      <td>degrees celcius Mumbai wear pants   difficult ...</td>\n",
       "      <td>[-0.018360869565217392, 0.006960869565217391, ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4475</th>\n",
       "      <td>elisabeth.rioux</td>\n",
       "      <td>False</td>\n",
       "      <td>Monday</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>121036.4</td>\n",
       "      <td>Missing much island think beautiful Caribbean ...</td>\n",
       "      <td>[-0.011155555555555556, 0.0003999999999999989,...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16092</th>\n",
       "      <td>wristtakers</td>\n",
       "      <td>False</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Determination beaded bracelet earth tones Wris...</td>\n",
       "      <td>[0.09723333333333334, 0.0016555555555555568, -...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15011</th>\n",
       "      <td>the_fabcloset</td>\n",
       "      <td>False</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>Sunday stop smell flowers Unless allergies sto...</td>\n",
       "      <td>[-0.04809411764705882, 0.010547058823529413, -...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15266</th>\n",
       "      <td>trainforfitspo</td>\n",
       "      <td>False</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1640.4</td>\n",
       "      <td>Email  celinefrazierfitnessgmailcom head pag...</td>\n",
       "      <td>[-0.01166666666666667, 0.009093333333333333, -...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>colerise</td>\n",
       "      <td>False</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>end road western point jamaica taken rum hand ...</td>\n",
       "      <td>[0.0178, -0.002629999999999998, -0.03778, -0.0...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5202</th>\n",
       "      <td>finn</td>\n",
       "      <td>False</td>\n",
       "      <td>Monday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11042.0</td>\n",
       "      <td>Border country Wales meets England Home</td>\n",
       "      <td>[0.06026666666666667, -0.05933333333333333, 0....</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9274 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 alias  multipleImage    weekday  numberLikesCategory  \\\n",
       "4472   elisabeth.rioux          False   Saturday                   10   \n",
       "16495          _picolo          False     Friday                   10   \n",
       "4482   elisabeth.rioux          False  Wednesday                   10   \n",
       "4480   elisabeth.rioux          False     Monday                   10   \n",
       "4475   elisabeth.rioux          False     Monday                   10   \n",
       "...                ...            ...        ...                  ...   \n",
       "16092      wristtakers          False     Sunday                    1   \n",
       "15011    the_fabcloset          False     Sunday                    1   \n",
       "15266   trainforfitspo          False     Sunday                    1   \n",
       "3249          colerise          False   Thursday                    1   \n",
       "5202              finn          False     Monday                    1   \n",
       "\n",
       "       amount_relevant_tags  moving_avg  \\\n",
       "4472                      0    119771.0   \n",
       "16495                     0     70003.8   \n",
       "4482                      0    111266.4   \n",
       "4480                      0    116196.8   \n",
       "4475                      0    121036.4   \n",
       "...                     ...         ...   \n",
       "16092                     1        -1.0   \n",
       "15011                     1       122.0   \n",
       "15266                     0      1640.4   \n",
       "3249                      0        -1.0   \n",
       "5202                      0     11042.0   \n",
       "\n",
       "                                    descriptionProcessed  \\\n",
       "4472   OFFICIALLY PARENTS    Yes Jonathan came live C...   \n",
       "16495  Going watch Beauty Beast tonight    Hoping wor...   \n",
       "4482   days Ill way see love life India hes reason ha...   \n",
       "4480   degrees celcius Mumbai wear pants   difficult ...   \n",
       "4475   Missing much island think beautiful Caribbean ...   \n",
       "...                                                  ...   \n",
       "16092  Determination beaded bracelet earth tones Wris...   \n",
       "15011  Sunday stop smell flowers Unless allergies sto...   \n",
       "15266    Email  celinefrazierfitnessgmailcom head pag...   \n",
       "3249   end road western point jamaica taken rum hand ...   \n",
       "5202             Border country Wales meets England Home   \n",
       "\n",
       "                                       descriptionVector language  \n",
       "4472   [-0.015670370370370366, 0.01253703703703704, -...       en  \n",
       "16495  [-0.042608695652173914, -0.024921739130434783,...       en  \n",
       "4482   [-0.04034375, -0.010975, -0.04198125, -0.05997...       en  \n",
       "4480   [-0.018360869565217392, 0.006960869565217391, ...       en  \n",
       "4475   [-0.011155555555555556, 0.0003999999999999989,...       en  \n",
       "...                                                  ...      ...  \n",
       "16092  [0.09723333333333334, 0.0016555555555555568, -...       en  \n",
       "15011  [-0.04809411764705882, 0.010547058823529413, -...       en  \n",
       "15266  [-0.01166666666666667, 0.009093333333333333, -...       en  \n",
       "3249   [0.0178, -0.002629999999999998, -0.03778, -0.0...       en  \n",
       "5202   [0.06026666666666667, -0.05933333333333333, 0....       en  \n",
       "\n",
       "[9274 rows x 9 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_keep = ['alias', 'multipleImage', 'weekday', 'numberLikesCategory', 'amount_relevant_tags', 'moving_avg', 'descriptionProcessed','descriptionVector', 'language']\n",
    "posts_df = posts_df[posts_df['language'] == 'en']\n",
    "posts_df = posts_df[columns_to_keep]\n",
    "posts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alias</th>\n",
       "      <th>multipleImage</th>\n",
       "      <th>weekday</th>\n",
       "      <th>numberLikesCategory</th>\n",
       "      <th>amount_relevant_tags</th>\n",
       "      <th>moving_avg</th>\n",
       "      <th>descriptionProcessed</th>\n",
       "      <th>descriptionVector</th>\n",
       "      <th>language</th>\n",
       "      <th>numberPosts</th>\n",
       "      <th>numberFollowers</th>\n",
       "      <th>numberFollowing</th>\n",
       "      <th>website_available</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elisabeth.rioux</td>\n",
       "      <td>False</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>119771.0</td>\n",
       "      <td>OFFICIALLY PARENTS    Yes Jonathan came live C...</td>\n",
       "      <td>[-0.015670370370370366, 0.01253703703703704, -...</td>\n",
       "      <td>en</td>\n",
       "      <td>554</td>\n",
       "      <td>990729</td>\n",
       "      <td>266</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elisabeth.rioux</td>\n",
       "      <td>False</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>111266.4</td>\n",
       "      <td>days Ill way see love life India hes reason ha...</td>\n",
       "      <td>[-0.04034375, -0.010975, -0.04198125, -0.05997...</td>\n",
       "      <td>en</td>\n",
       "      <td>554</td>\n",
       "      <td>990729</td>\n",
       "      <td>266</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>elisabeth.rioux</td>\n",
       "      <td>False</td>\n",
       "      <td>Monday</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>116196.8</td>\n",
       "      <td>degrees celcius Mumbai wear pants   difficult ...</td>\n",
       "      <td>[-0.018360869565217392, 0.006960869565217391, ...</td>\n",
       "      <td>en</td>\n",
       "      <td>554</td>\n",
       "      <td>990729</td>\n",
       "      <td>266</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elisabeth.rioux</td>\n",
       "      <td>False</td>\n",
       "      <td>Monday</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>121036.4</td>\n",
       "      <td>Missing much island think beautiful Caribbean ...</td>\n",
       "      <td>[-0.011155555555555556, 0.0003999999999999989,...</td>\n",
       "      <td>en</td>\n",
       "      <td>554</td>\n",
       "      <td>990729</td>\n",
       "      <td>266</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>elisabeth.rioux</td>\n",
       "      <td>False</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>123242.8</td>\n",
       "      <td>Small nose small cheeks big lips big eyes flee...</td>\n",
       "      <td>[-0.02209454545454545, -0.038102727272727274, ...</td>\n",
       "      <td>en</td>\n",
       "      <td>554</td>\n",
       "      <td>990729</td>\n",
       "      <td>266</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9264</th>\n",
       "      <td>wristtakers</td>\n",
       "      <td>False</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117.6</td>\n",
       "      <td>New beautiful colors Check silicon beaded brac...</td>\n",
       "      <td>[0.07342499999999999, -0.020012500000000003, 0...</td>\n",
       "      <td>en</td>\n",
       "      <td>15</td>\n",
       "      <td>148876</td>\n",
       "      <td>2085</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9267</th>\n",
       "      <td>wristtakers</td>\n",
       "      <td>False</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>103.6</td>\n",
       "      <td>new bracelets going website tomorrow sure chec...</td>\n",
       "      <td>[0.013291666666666665, 0.014916666666666663, -...</td>\n",
       "      <td>en</td>\n",
       "      <td>15</td>\n",
       "      <td>148876</td>\n",
       "      <td>2085</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9268</th>\n",
       "      <td>wristtakers</td>\n",
       "      <td>False</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117.6</td>\n",
       "      <td>Dress AppleWatch wristtakers silicon beaded br...</td>\n",
       "      <td>[0.09100000000000001, -0.017183333333333335, 0...</td>\n",
       "      <td>en</td>\n",
       "      <td>15</td>\n",
       "      <td>148876</td>\n",
       "      <td>2085</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9269</th>\n",
       "      <td>wristtakers</td>\n",
       "      <td>False</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>103.6</td>\n",
       "      <td>stock Equality rainbow silicon beaded bracelet...</td>\n",
       "      <td>[0.07602500000000001, -0.036575, 0.0325875, 0....</td>\n",
       "      <td>en</td>\n",
       "      <td>15</td>\n",
       "      <td>148876</td>\n",
       "      <td>2085</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9270</th>\n",
       "      <td>wristtakers</td>\n",
       "      <td>False</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>117.6</td>\n",
       "      <td>stock Wristtakers accessories</td>\n",
       "      <td>[-0.026366666666666667, -0.05516666666666667, ...</td>\n",
       "      <td>en</td>\n",
       "      <td>15</td>\n",
       "      <td>148876</td>\n",
       "      <td>2085</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6305 rows √ó 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                alias  multipleImage    weekday  numberLikesCategory  \\\n",
       "0     elisabeth.rioux          False   Saturday                   10   \n",
       "1     elisabeth.rioux          False  Wednesday                   10   \n",
       "2     elisabeth.rioux          False     Monday                   10   \n",
       "3     elisabeth.rioux          False     Monday                   10   \n",
       "4     elisabeth.rioux          False    Tuesday                   10   \n",
       "...               ...            ...        ...                  ...   \n",
       "9264      wristtakers          False   Saturday                    1   \n",
       "9267      wristtakers          False     Friday                    1   \n",
       "9268      wristtakers          False   Saturday                    1   \n",
       "9269      wristtakers          False     Friday                    1   \n",
       "9270      wristtakers          False   Saturday                    1   \n",
       "\n",
       "      amount_relevant_tags  moving_avg  \\\n",
       "0                        0    119771.0   \n",
       "1                        0    111266.4   \n",
       "2                        0    116196.8   \n",
       "3                        0    121036.4   \n",
       "4                        0    123242.8   \n",
       "...                    ...         ...   \n",
       "9264                     0       117.6   \n",
       "9267                     0       103.6   \n",
       "9268                     0       117.6   \n",
       "9269                     0       103.6   \n",
       "9270                     1       117.6   \n",
       "\n",
       "                                   descriptionProcessed  \\\n",
       "0     OFFICIALLY PARENTS    Yes Jonathan came live C...   \n",
       "1     days Ill way see love life India hes reason ha...   \n",
       "2     degrees celcius Mumbai wear pants   difficult ...   \n",
       "3     Missing much island think beautiful Caribbean ...   \n",
       "4     Small nose small cheeks big lips big eyes flee...   \n",
       "...                                                 ...   \n",
       "9264  New beautiful colors Check silicon beaded brac...   \n",
       "9267  new bracelets going website tomorrow sure chec...   \n",
       "9268  Dress AppleWatch wristtakers silicon beaded br...   \n",
       "9269  stock Equality rainbow silicon beaded bracelet...   \n",
       "9270                      stock Wristtakers accessories   \n",
       "\n",
       "                                      descriptionVector language  numberPosts  \\\n",
       "0     [-0.015670370370370366, 0.01253703703703704, -...       en          554   \n",
       "1     [-0.04034375, -0.010975, -0.04198125, -0.05997...       en          554   \n",
       "2     [-0.018360869565217392, 0.006960869565217391, ...       en          554   \n",
       "3     [-0.011155555555555556, 0.0003999999999999989,...       en          554   \n",
       "4     [-0.02209454545454545, -0.038102727272727274, ...       en          554   \n",
       "...                                                 ...      ...          ...   \n",
       "9264  [0.07342499999999999, -0.020012500000000003, 0...       en           15   \n",
       "9267  [0.013291666666666665, 0.014916666666666663, -...       en           15   \n",
       "9268  [0.09100000000000001, -0.017183333333333335, 0...       en           15   \n",
       "9269  [0.07602500000000001, -0.036575, 0.0325875, 0....       en           15   \n",
       "9270  [-0.026366666666666667, -0.05516666666666667, ...       en           15   \n",
       "\n",
       "      numberFollowers  numberFollowing  website_available  \n",
       "0              990729              266                  1  \n",
       "1              990729              266                  1  \n",
       "2              990729              266                  1  \n",
       "3              990729              266                  1  \n",
       "4              990729              266                  1  \n",
       "...               ...              ...                ...  \n",
       "9264           148876             2085                  1  \n",
       "9267           148876             2085                  1  \n",
       "9268           148876             2085                  1  \n",
       "9269           148876             2085                  1  \n",
       "9270           148876             2085                  1  \n",
       "\n",
       "[6305 rows x 13 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = posts_df.merge(profiles_df, on='alias', how='inner')\n",
    "merged_df = merged_df[merged_df['moving_avg'] != -1]\n",
    "#merged_df.to_csv('merged_data.csv', index=True, sep=';')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6305, 500)\n"
     ]
    }
   ],
   "source": [
    "#by default the vectorizer conerts the text to lower case and uses word-level tokenization\n",
    "# Create an instance of CountVectorizer with max_features set to 500 (this is what they did in the tds implementation)\n",
    "vec = CountVectorizer(max_features=500)\n",
    "\n",
    "\n",
    "# Transform the \"descriptionProcessed\" column into a matrix of token counts\n",
    "description_counts = vec.fit_transform(merged_df['descriptionProcessed'])\n",
    "\n",
    "# Convert the matrix to an array\n",
    "description_counts_array = description_counts.toarray()\n",
    "\n",
    "word_vectors = pd.DataFrame(data=description_counts_array,columns = vec.get_feature_names_out())\n",
    "print(word_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([merged_df.reset_index(drop=True), word_vectors.reset_index(drop=True)], axis=1)\n",
    "final_df = final_df.drop(\"language\", axis=1)\n",
    "final_df.to_csv('final_data.csv', index=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alias</th>\n",
       "      <th>multipleImage</th>\n",
       "      <th>weekday</th>\n",
       "      <th>numberLikesCategory</th>\n",
       "      <th>amount_relevant_tags</th>\n",
       "      <th>moving_avg</th>\n",
       "      <th>descriptionProcessed</th>\n",
       "      <th>descriptionVector</th>\n",
       "      <th>numberPosts</th>\n",
       "      <th>numberFollowers</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>yoga</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "      <th>youtube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elisabeth.rioux</td>\n",
       "      <td>False</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>119771.0</td>\n",
       "      <td>OFFICIALLY PARENTS    Yes Jonathan came live C...</td>\n",
       "      <td>[-0.015670370370370366, 0.01253703703703704, -...</td>\n",
       "      <td>554</td>\n",
       "      <td>990729</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elisabeth.rioux</td>\n",
       "      <td>False</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>111266.4</td>\n",
       "      <td>days Ill way see love life India hes reason ha...</td>\n",
       "      <td>[-0.04034375, -0.010975, -0.04198125, -0.05997...</td>\n",
       "      <td>554</td>\n",
       "      <td>990729</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>elisabeth.rioux</td>\n",
       "      <td>False</td>\n",
       "      <td>Monday</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>116196.8</td>\n",
       "      <td>degrees celcius Mumbai wear pants   difficult ...</td>\n",
       "      <td>[-0.018360869565217392, 0.006960869565217391, ...</td>\n",
       "      <td>554</td>\n",
       "      <td>990729</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elisabeth.rioux</td>\n",
       "      <td>False</td>\n",
       "      <td>Monday</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>121036.4</td>\n",
       "      <td>Missing much island think beautiful Caribbean ...</td>\n",
       "      <td>[-0.011155555555555556, 0.0003999999999999989,...</td>\n",
       "      <td>554</td>\n",
       "      <td>990729</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>elisabeth.rioux</td>\n",
       "      <td>False</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>123242.8</td>\n",
       "      <td>Small nose small cheeks big lips big eyes flee...</td>\n",
       "      <td>[-0.02209454545454545, -0.038102727272727274, ...</td>\n",
       "      <td>554</td>\n",
       "      <td>990729</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6300</th>\n",
       "      <td>wristtakers</td>\n",
       "      <td>False</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117.6</td>\n",
       "      <td>New beautiful colors Check silicon beaded brac...</td>\n",
       "      <td>[0.07342499999999999, -0.020012500000000003, 0...</td>\n",
       "      <td>15</td>\n",
       "      <td>148876</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6301</th>\n",
       "      <td>wristtakers</td>\n",
       "      <td>False</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>103.6</td>\n",
       "      <td>new bracelets going website tomorrow sure chec...</td>\n",
       "      <td>[0.013291666666666665, 0.014916666666666663, -...</td>\n",
       "      <td>15</td>\n",
       "      <td>148876</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6302</th>\n",
       "      <td>wristtakers</td>\n",
       "      <td>False</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117.6</td>\n",
       "      <td>Dress AppleWatch wristtakers silicon beaded br...</td>\n",
       "      <td>[0.09100000000000001, -0.017183333333333335, 0...</td>\n",
       "      <td>15</td>\n",
       "      <td>148876</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6303</th>\n",
       "      <td>wristtakers</td>\n",
       "      <td>False</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>103.6</td>\n",
       "      <td>stock Equality rainbow silicon beaded bracelet...</td>\n",
       "      <td>[0.07602500000000001, -0.036575, 0.0325875, 0....</td>\n",
       "      <td>15</td>\n",
       "      <td>148876</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6304</th>\n",
       "      <td>wristtakers</td>\n",
       "      <td>False</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>117.6</td>\n",
       "      <td>stock Wristtakers accessories</td>\n",
       "      <td>[-0.026366666666666667, -0.05516666666666667, ...</td>\n",
       "      <td>15</td>\n",
       "      <td>148876</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6305 rows √ó 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                alias  multipleImage    weekday  numberLikesCategory  \\\n",
       "0     elisabeth.rioux          False   Saturday                   10   \n",
       "1     elisabeth.rioux          False  Wednesday                   10   \n",
       "2     elisabeth.rioux          False     Monday                   10   \n",
       "3     elisabeth.rioux          False     Monday                   10   \n",
       "4     elisabeth.rioux          False    Tuesday                   10   \n",
       "...               ...            ...        ...                  ...   \n",
       "6300      wristtakers          False   Saturday                    1   \n",
       "6301      wristtakers          False     Friday                    1   \n",
       "6302      wristtakers          False   Saturday                    1   \n",
       "6303      wristtakers          False     Friday                    1   \n",
       "6304      wristtakers          False   Saturday                    1   \n",
       "\n",
       "      amount_relevant_tags  moving_avg  \\\n",
       "0                        0    119771.0   \n",
       "1                        0    111266.4   \n",
       "2                        0    116196.8   \n",
       "3                        0    121036.4   \n",
       "4                        0    123242.8   \n",
       "...                    ...         ...   \n",
       "6300                     0       117.6   \n",
       "6301                     0       103.6   \n",
       "6302                     0       117.6   \n",
       "6303                     0       103.6   \n",
       "6304                     1       117.6   \n",
       "\n",
       "                                   descriptionProcessed  \\\n",
       "0     OFFICIALLY PARENTS    Yes Jonathan came live C...   \n",
       "1     days Ill way see love life India hes reason ha...   \n",
       "2     degrees celcius Mumbai wear pants   difficult ...   \n",
       "3     Missing much island think beautiful Caribbean ...   \n",
       "4     Small nose small cheeks big lips big eyes flee...   \n",
       "...                                                 ...   \n",
       "6300  New beautiful colors Check silicon beaded brac...   \n",
       "6301  new bracelets going website tomorrow sure chec...   \n",
       "6302  Dress AppleWatch wristtakers silicon beaded br...   \n",
       "6303  stock Equality rainbow silicon beaded bracelet...   \n",
       "6304                      stock Wristtakers accessories   \n",
       "\n",
       "                                      descriptionVector  numberPosts  \\\n",
       "0     [-0.015670370370370366, 0.01253703703703704, -...          554   \n",
       "1     [-0.04034375, -0.010975, -0.04198125, -0.05997...          554   \n",
       "2     [-0.018360869565217392, 0.006960869565217391, ...          554   \n",
       "3     [-0.011155555555555556, 0.0003999999999999989,...          554   \n",
       "4     [-0.02209454545454545, -0.038102727272727274, ...          554   \n",
       "...                                                 ...          ...   \n",
       "6300  [0.07342499999999999, -0.020012500000000003, 0...           15   \n",
       "6301  [0.013291666666666665, 0.014916666666666663, -...           15   \n",
       "6302  [0.09100000000000001, -0.017183333333333335, 0...           15   \n",
       "6303  [0.07602500000000001, -0.036575, 0.0325875, 0....           15   \n",
       "6304  [-0.026366666666666667, -0.05516666666666667, ...           15   \n",
       "\n",
       "      numberFollowers  ...  year  years  yes  yesterday  yet  yoga  you  \\\n",
       "0              990729  ...     0      0    1          0    0     0    0   \n",
       "1              990729  ...     0      0    0          0    0     0    0   \n",
       "2              990729  ...     0      0    0          0    0     0    0   \n",
       "3              990729  ...     0      0    0          0    0     0    0   \n",
       "4              990729  ...     0      0    0          1    0     0    0   \n",
       "...               ...  ...   ...    ...  ...        ...  ...   ...  ...   \n",
       "6300           148876  ...     0      0    0          0    0     0    0   \n",
       "6301           148876  ...     0      0    0          0    0     0    0   \n",
       "6302           148876  ...     0      0    0          0    0     0    0   \n",
       "6303           148876  ...     0      0    0          0    0     0    0   \n",
       "6304           148876  ...     0      0    0          0    0     0    0   \n",
       "\n",
       "      young  youre  youtube  \n",
       "0         0      0        1  \n",
       "1         0      0        0  \n",
       "2         0      0        0  \n",
       "3         0      0        0  \n",
       "4         0      0        0  \n",
       "...     ...    ...      ...  \n",
       "6300      0      0        0  \n",
       "6301      0      0        0  \n",
       "6302      0      0        0  \n",
       "6303      0      0        0  \n",
       "6304      0      0        0  \n",
       "\n",
       "[6305 rows x 512 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
